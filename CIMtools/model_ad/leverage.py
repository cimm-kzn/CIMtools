# -*- coding: utf-8 -*-
#
#
#  Copyright 2019 Assima Rakhimbekova <asima.astana@outlook.com>
#  This file is part of CIMtools.
#
#  CIMtools is free software; you can redistribute it and/or modify
#  it under the terms of the GNU Affero General Public License as published by
#  the Free Software Foundation; either version 3 of the License, or
#  (at your option) any later version.
#
#  This program is distributed in the hope that it will be useful,
#  but WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU Affero General Public License for more details.
#
#  You should have received a copy of the GNU General Public License
#  along with this program; if not, see <https://www.gnu.org/licenses/>.
#
import numpy as np
from sklearn.base import BaseEstimator
from sklearn.utils.validation import check_array, check_is_fitted
from sklearn.ensemble import RandomForestRegressor


class Leverage(BaseEstimator):
    """ Distance-based method
    The model space can be represented by a two-dimensional matrix comprising n chemicals (rows) and
    k variables (columns), called the descriptor matrix (X). The leverage of a chemical provides a measure of the
    distance of the chemical from the centroid of X. Chemicals close to the centroid are less influential in model
    building than are extreme points. The leverages of all chemicals in the data set are generated by manipulating X
    according to Equation 1, to give the so-called Influence Matrix or Hat Matrix (H).

    H = X(XTX)–1 XT (Equation 1)

    where X is the descriptor matrix, XT is the transpose of X, and (A)–1 is the inverse of matrix A, where A = (XTX).

    The leverages or hat values (hi) of the chemicals (i) in the descriptor space are the diagonal elements of H,
    and can be computed by Equation 2.

    hii = xiT(XTX)–1 xi (Equation 2)

    where xi is the descriptor row-vector of the query chemical.
    A “warning leverage” (h*) is generally fixed at 3p/n, where n is the number of training chemicals, and p
    the number of model variables plus one. A chemical with high leverage in the training set greatly influences
    the regression line: the fitted regression line is forced near to the observed value and its residual
    (observed-predicted value) is small, so the chemical does not appear to be an outlier, even though it may actually
    be outside the AD. In contrast, if a chemical in the test set has a hat value greater than the warning leverage h*,
    this means that the prediction is the result of substantial extrapolation and therefore may not be reliable.

    Пояснение
    -----------

    h* может быть найден на внутренней кросс-валидации.
    В данном классе в методе predict на выходе выдаются 2 array: в первом случае в качестве отсечки используется
    найденная на внутренней кросс-валидации, вторая - h* = 3p/n

    Пример
    ------
     # ************************************************** LEVERAGE **************************************************
        Lev_model = Leverage(ad='leverage', metric=score, reg_model=est.best_estimator_).fit(X_train, Y_train)
        Lev_val = Lev_model.predict_proba(X_test)
        Lev_threshold = threshold(ad='leverage', X=X_train, y=Y_pr_ts, metric=score, reg_model=est.best_estimator_)
        AD_Lev_cv = Lev_val <= Lev_threshold['z']
        res(AD_res, 12, AD_Lev_cv, FC=AD_FC, RTC_deep_1=AD_RTC_deep1)
        AD_Lev = Lev_val <= (3 * (1 + X_train.shape[1]) / X_train.shape[0])
        res(AD_res, 15, AD_Lev, FC=AD_FC, RTC_deep_1=AD_RTC_deep1)
    """

    def __init__(self, ad='leverage', metric='ba', reg_model=RandomForestRegressor(n_estimators=500, random_state=1)):
        self.ad = ad
        self.metric = metric # по умолчанию стоит метрика 'ba'
        self.reg_model = reg_model # необходима для нахождения отсечки, если пользователь не задает регресиооную модель,
        # то по умолчанию будет RandomForestRegressor(n_estimators=500, random_state=1). Можно также добавить внутри
        # gridSearchCV если надо

    def fit(self, X, y=None):
        """ Обучение заключается в построение обратной матрицы и нахождения отсечки

        Parameters
        ----------
        X : array-like or sparse matrix, shape (n_samples, n_features)
            The input samples. Use ``dtype=np.float32`` for maximum
            efficiency.
        y : array-like, shape = [n_samples] or [n_samples, n_outputs]
            The target values (real numbers in regression).
        Returns
        -------
        self : object
            Returns self.
            self.X нужен будет для того чтобы найти отсечку
        """
        self.X = check_array(X)
        if y is not None:
            self.y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None) # нужен не для обучения, а
            # для нахождения отсечки
        self._X = np.column_stack(((np.ones(self.X.shape[0])), self.X)) # Матрица, первый столбец которой состоит из
        # единиц, а элемент на пересечении i-ой строки и (j+1)-ого стобца равен значению j-ого дескпритора для i-ого
        # химического объекта из обучающей выбоки
        # NB! self.X также нужен для нахождения отсечки!!!
        unit_matrix = np.eye(self._X.shape[1]).dot(1e-8) # единичная матрица чтобы прибавить ее на матрицу ниже для того
        # чтобы определитель матрицы можно было посчитать и поэтому найти опртаную матрицу
        influence_matrix = self._X.T.dot(self._X) + unit_matrix # XTX + E
        self.inverse_influence_matrix = np.linalg.inv(influence_matrix) # обратная матрица
        return self

    def predict_proba(self, X):
        """Predict if a particular sample is an outlier or not.

            Parameters
           ----------
           X : array-like or sparse matrix, shape (n_samples, n_features)
               The input samples. Internally, it will be converted to
               ``dtype=np.float32`` and if a sparse matrix is provided
               to a sparse ``csr_matrix``.

           Returns
           -------
           is_inlier : array, shape (n_samples,)
               For each observations, tells whether or not (True or False) it should
               be considered as an inlier according to the fitted model.
        """
        X = check_array(X)
        check_is_fitted(self, ['inverse_influence_matrix'])
        X = np.column_stack(((np.ones(X.shape[0])), X))
        return np.array([X[i, :].dot(self.inverse_influence_matrix).dot(X[i, :]) for i in range(X.shape[0])])
